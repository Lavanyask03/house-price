adf <- read.csv("Cardata.csv", header = TRUE)
summary(adf)
hist(adf$disp)
hist(adf$HP)
hist(adf$wt)
pairs(~mpg + disp + HP + wt, data = adf)
plot(adf$mpg,adf$disp)
amultiple_model <- lm(mpg~., data = adf)
summary(amultiple_model)
# import dataset into df
df <- read.csv("House_Price.csv",header = TRUE)
View(df)
str(df)
summary(df)
# univariate analysis
hist(df$crime_rate)
# draw scatter plots to view distribution of data
pairs(~price + crime_rate + n_hot_rooms + rainfall, data = df)
# plotting barplots
barplot(table(df$airport))
barplot(table(df$waterbody))
barplot(table(df$bus_ter))
# outlier treatment
# capping and flooring
uv <- 3*quantile(df$n_hot_rooms, 0.99)
df$n_hot_rooms[df$n_hot_rooms > uv] <- uv # change the max value
summary(df$n_hot_rooms)
lv <- 0.3*quantile(df$rainfall, 0.01)
df$rainfall[df$rainfall < lv] <- lv # change the min value
summary(df$rainfall)
pairs(~price + n_hot_rooms + rainfall, data = df)
# missing value imputation
# replace NA values in n_hos_beds with the mean of n_hos_beds
mean_value <- mean(df$n_hos_beds, na.rm = TRUE)
# to see all NA values
which(is.na(df$n_hos_beds)) # 51 113 216 261 360 404 417 497
df$n_hos_beds[is.na(df$n_hos_beds)] <- mean_value
summary(df$n_hos_beds)
which(is.na(df$n_hos_beds)) # integer(0)
# variable transformation
pairs(~price + crime_rate, data = df)
plot(df$price, df$crime_rate)
# the plot is similar to log(x) plot
# change log(x) to log(1+x)
df$crime_rate <- log(1 + df$crime_rate)
# make dist1, dist2, dist3 and dist4 into one variable
df$avg_dist <- (df$dist1 + df$dist2 + df$dist3 + df$dist4)/4
# remove dist1, dist2, dist3 and dist4
df2 <- df[,-7:-10]
df <- df2
rm(df2)
# remove bus_ter
df <- df[,-14]
# dummy variable creation to change categorical values -> numerical values
install.packages("dummies")
df <- dummy.data.frame(df)
View(df)
# remove airportNO and waterbodyNone
df <- df[,-9]
df <- df[,-14]
library(dummies)
df <- dummy.data.frame(df)
View(df)
# remove airportNO and waterbodyNone
df <- df[,-9]
df <- df[,-14]
# correlation matrix
round(cor(df),2)
# OBSERVATIONS from correlation matrix
# air_qual and parks are highly correlated - 0.92
# delete parks - low correlation with price
df <- df[,-16]
# simple linear regression
simple_model <- lm(price~room_num, data = df)
summary(simple_model)
# Residual standard error: 6.597 on 504 degrees of freedom
# Multiple R-squared:  0.4848,	Adjusted R-squared:  0.4838
# F-statistic: 474.3 on 1 and 504 DF,  p-value: < 2.2e-16
# less p value -> there is a relationship b/w room_num & price
plot(df$room_num,df$price) # scatterplot
abline(simple_model)
# multiple linear regression
multiple_model <- lm(price~., data = df)
summary(multiple_model)
# test-train split
install.packages("caTools")
library(caTools)
set.seed(0)
split <- sample.split(df, SplitRatio = 0.8)
training_set <- subset(df, split == TRUE)
test_set <- subset(df, split == FALSE)
# train the model
lm_a <- lm(price~., data = training_set)
train_a <- predict(lm_a, training_set)
test_a <- predict(lm_a, test_set)
# get the mean squared error
mean((training_set$price - train_a)^2) # 20.66384
mean((test_set$price - test_a)^2) # 33.04607
# subset selection
install.packages("leaps")
library(leaps)
# 1. best subset selection
lm_best <- regsubsets(price~., data = df, nvmax = 15)
summary(lm_best)
summary(lm_best)$adjr2
which.max(summary(lm_best)$adjr2)
coef_best <- coef(lm_best,8)
# 2. forward step wise selection
lm_fwd <- regsubsets(price~., data = df, nvmax = 15, method = "forward")
coef_fwd <- coef(lm_fwd,which.max(summary(lm_fwd)$adjr2))
# 3. backward step wise selection
lm_bkwd <- regsubsets(price~., data = df, nvmax = 15, method = "backward")
coef_bkwd <- coef(lm_bkwd,which.max(summary(lm_bkwd)$adjr2))
coef_best
coef_fwd
coef_bkwd
# ridge and lasso regression
# x => independent var, y => dependent var
x <- model.matrix(price~., data = df)[,-1]
y <- df$price
# create a grid of lambda from 10^10 - 10^-2
grid <- 10^seq(10,-2,length = 100)
grid
# ridge regression
lm_ridge <- glmnet(x,y,alpha = 0, lambda = grid)
cv_fit <- cv.glmnet(x,y,alpha = 0, lambda = grid)
library(gbm)
# ridge regression
lm_ridge <- glmnet(x,y,alpha = 0, lambda = grid)
# ridge regression
install.packages("glmnet")
library(glmnet)
lm_ridge <- glmnet(x,y,alpha = 0, lambda = grid)
cv_fit <- cv.glmnet(x,y,alpha = 0, lambda = grid)
plot(cv_fit)
opt_lambda <- cv_fit$lambda.min # get min MSE
tss <- sum((y-mean(y))^2)
y_a <- predict(lm_ridge, s = opt_lambda, newx = x)
rss <- sum((y_a - y)^2)
r2 <- 1 - rss/tss
# lasso regression
lm_lasso <- glmnet(x,y,alpha = 1, lambda = grid)
cv_fitl <- cv.glmnet(x,y,alpha = 1, lambda = grid)
plot(cv_fitl)
opt_lambdal <- cv_fitl$lambda.min # get min MSE
y_al <- predict(lm_lasso, s = opt_lambdal, newx = x)
rssl <- sum((y_al - y)^2)
r2l <- 1 - rssl/tss
